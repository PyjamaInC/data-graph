I'll help you review the LangGraph and LangChain implementation in your plan.md file to ensure it aligns with the latest documentation. Let me first check the current LangGraph and LangChain documentation to understand the most recent patterns and best practices.

I'll select the latest version of LangGraph to get the most current documentation.

Let me try the main LangGraph library ID without the version.

Now let me check LangChain documentation as well to understand their current patterns.

I'll get the latest LangChain documentation to understand the current patterns for multi-agent systems and tool usage.

Now let me analyze your plan.md file and provide a comprehensive review of the LangGraph and LangChain implementation to ensure it aligns with the latest documentation.

## Review of Your LangGraph + LangChain Implementation

After reviewing your plan.md file against the latest LangGraph and LangChain documentation, I've identified several areas where your implementation needs updates to align with current best practices. Let me walk you through the necessary changes step by step.

### 1. **State Definition Pattern Updates**

Your current implementation uses a basic `TypedDict` for state management. While this is still valid, the latest LangGraph patterns show more sophisticated state handling. Here's how to update your `AnalysisState`:

```python
from typing import Annotated, List, Dict, Any
from typing_extensions import TypedDict
from langchain_core.messages import BaseMessage, AnyMessage
from langgraph.graph.message import add_messages
import operator

# Updated state definition with proper annotations
class AnalysisState(TypedDict):
    """State for multi-agent analysis workflow"""
    # Using Annotated with add_messages for proper message handling
    messages: Annotated[List[AnyMessage], add_messages]
    
    # Dataset information
    dataset_info: Dict[str, Any]
    
    # Results from each agent - using regular dict updates
    exploration_results: Dict[str, Any]
    relationship_results: Dict[str, Any]
    pattern_results: Dict[str, Any]
    business_insights: Dict[str, Any]
    
    # Workflow control
    current_agent: str
    analysis_complete: bool
    
    # For aggregating results across agents
    loop_step: Annotated[int, operator.add]
    
    # Knowledge graph reference - store as serializable data
    kg_metadata: Dict[str, Any]  # Instead of storing the actual kg_builder
```

### 2. **Agent Creation Pattern Updates**

The latest LangGraph documentation shows that `create_openai_tools_agent` from LangChain is deprecated in favor of LangGraph's `create_react_agent`. Here's the updated pattern:

```python
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# Define tools using the @tool decorator
@tool
def query_knowledge_graph(query: str) -> str:
    """Query the knowledge graph for relationship information"""
    # Tool implementation
    return result

@tool
def execute_data_analysis(analysis_request: str) -> str:
    """Execute data analysis code and return results"""
    # Tool implementation
    return result

# Create the agent nodes
def create_data_exploration_agent():
    """Create data exploration agent using create_react_agent"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    tools = [query_knowledge_graph, execute_data_analysis]
    
    # Use create_react_agent instead of create_openai_tools_agent
    agent = create_react_agent(
        llm,
        tools,
        prompt="You are an expert data scientist specializing in exploratory data analysis..."
    )
    
    return agent
```

### 3. **Node Functions Update**

Your node functions need to be updated to work with the new patterns. Here's the corrected approach:

```python
from langgraph.types import Command
from typing import Literal

def data_exploration_node(state: AnalysisState) -> Dict[str, Any]:
    """Node for data exploration analysis"""
    # Get the agent
    agent = create_data_exploration_agent()
    
    # Prepare the input
    messages = state["messages"]
    
    # Execute the agent - agents now return message responses
    response = agent.invoke({"messages": messages})
    
    # Extract results from the agent's response
    exploration_results = {
        "analysis_complete": True,
        "key_findings": extract_findings_from_messages(response["messages"]),
        "metrics": extract_metrics_from_messages(response["messages"])
    }
    
    # Return state updates
    return {
        "messages": response["messages"],
        "exploration_results": exploration_results,
        "current_agent": "relationship_analyst",
        "loop_step": 1
    }

# For more complex routing, use Command objects
def routing_node(state: AnalysisState) -> Command[Literal["explorer", "analyst", "miner", END]]:
    """Advanced routing with Command objects"""
    if state["loop_step"] > 3:
        return Command(goto=END)
    
    next_agent = determine_next_agent(state)
    return Command(
        goto=next_agent,
        update={"messages": [AIMessage(content="Proceeding to next analysis phase")]}
    )
```

### 4. **LangGraph Workflow Construction Update**

The workflow construction pattern has been refined. Here's the updated approach:

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

class LangGraphMultiAgentOrchestrator:
    def __init__(self):
        self.memory = MemorySaver()
        self.workflow = self._build_workflow()
        
    def _build_workflow(self) -> StateGraph:
        """Build the LangGraph workflow with latest patterns"""
        # Create the state graph with explicit state schema
        workflow = StateGraph(AnalysisState)
        
        # Add nodes - each node is a function that takes state and returns state updates
        workflow.add_node("data_explorer", data_exploration_node)
        workflow.add_node("relationship_analyst", relationship_analysis_node)
        workflow.add_node("pattern_miner", pattern_mining_node)
        workflow.add_node("business_synthesizer", business_synthesis_node)
        
        # Set entry point
        workflow.add_edge(START, "data_explorer")
        
        # Add conditional edges for dynamic routing
        workflow.add_conditional_edges(
            "data_explorer",
            lambda state: "relationship_analyst" if state["exploration_results"] else END
        )
        
        # Sequential edges
        workflow.add_edge("relationship_analyst", "pattern_miner")
        workflow.add_edge("pattern_miner", "business_synthesizer")
        workflow.add_edge("business_synthesizer", END)
        
        # Compile with checkpointer for state persistence
        return workflow.compile(checkpointer=self.memory)
```

### 5. **Tool Definition Best Practices**

Tools should be defined using the `@tool` decorator with proper type hints:

```python
from langchain_core.tools import tool
from pydantic import BaseModel, Field

# Define structured input/output for tools
class KGQueryInput(BaseModel):
    query: str = Field(description="Natural language query about the knowledge graph")
    filter_type: Optional[str] = Field(default=None, description="Filter by relationship type")

@tool(args_schema=KGQueryInput)
def query_knowledge_graph(query: str, filter_type: Optional[str] = None) -> str:
    """
    Query the knowledge graph for relationship information.
    
    Args:
        query: Natural language query about relationships
        filter_type: Optional filter by relationship type
        
    Returns:
        JSON string with query results
    """
    # Implementation using the global kg_builder instance
    # Note: In production, you'd pass this through a different mechanism
    results = perform_kg_query(query, filter_type)
    return json.dumps(results, default=str)
```

### 6. **Handling State Persistence**

Your current implementation stores the `kg_builder` directly in state, which won't work with LangGraph's serialization. Here's the correct approach:

```python
# Store kg_builder outside of state
class KnowledgeGraphManager:
    """Singleton manager for knowledge graph access"""
    _instance = None
    _kg_builder = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def set_kg_builder(self, kg_builder):
        self._kg_builder = kg_builder
    
    def get_kg_builder(self):
        return self._kg_builder

# In your orchestrator
class LangGraphMultiAgentOrchestrator:
    def __init__(self, kg_builder: EnhancedKnowledgeGraphBuilder):
        # Store kg_builder in manager, not in state
        KnowledgeGraphManager.get_instance().set_kg_builder(kg_builder)
        
        # Store only metadata in state
        self.kg_metadata = {
            'nodes': kg_builder.graph.number_of_nodes(),
            'edges': kg_builder.graph.number_of_edges(),
            'dataset_name': 'your_dataset'
        }
        
        self.workflow = self._build_workflow()
```

### 7. **Execution Pattern Updates**

The execution pattern should use the latest streaming capabilities:

```python
async def analyze_dataset(self, dataset_info: Dict[str, Any]) -> Dict[str, Any]:
    """Execute multi-agent analysis with streaming"""
    
    # Prepare initial state
    initial_state = {
        "messages": [HumanMessage(content="Starting knowledge graph analysis")],
        "dataset_info": dataset_info,
        "kg_metadata": self.kg_metadata,
        "exploration_results": {},
        "relationship_results": {},
        "pattern_results": {},
        "business_insights": {},
        "current_agent": "data_explorer",
        "analysis_complete": False,
        "loop_step": 0
    }
    
    # Configure thread for conversation memory
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    
    # Stream execution for real-time updates
    results = []
    async for event in self.workflow.astream(initial_state, config):
        print(f"Agent: {event.get('current_agent', 'unknown')}")
        results.append(event)
        
        # You can also stream individual updates
        if "messages" in event:
            last_message = event["messages"][-1]
            print(f"Latest: {last_message.content[:100]}...")
    
    # Get final state
    final_state = results[-1] if results else initial_state
    
    return self._compile_results(final_state)
```

### 8. **Integration with Latest LangChain Patterns**

For better integration with LangChain components:

```python
from langchain.chat_models import init_chat_model
from langgraph.prebuilt import ToolNode
from langchain_core.runnables import RunnableConfig

# Use init_chat_model for flexible model initialization
def create_llm(model_name: str = "gpt-4", temperature: float = 0):
    """Create LLM with fallback options"""
    return init_chat_model(
        model=f"openai:{model_name}",
        temperature=temperature,
        model_provider="openai"
    )

# Use ToolNode for better tool execution
def create_tool_node(tools: List):
    """Create a ToolNode for executing tools"""
    return ToolNode(tools=tools)

# Updated node with RunnableConfig support
def enhanced_data_exploration_node(state: AnalysisState, config: RunnableConfig) -> Dict[str, Any]:
    """Enhanced node with config support"""
    llm = create_llm()
    tools = [query_knowledge_graph, execute_data_analysis]
    
    # Create agent with config support
    agent = create_react_agent(llm, tools)
    
    # Execute with config for better observability
    response = agent.invoke(
        {"messages": state["messages"]},
        config=config
    )
    
    return {"messages": response["messages"]}
```

### Summary of Key Changes

The main updates needed for your implementation are:

1. **Use `create_react_agent`** instead of `create_openai_tools_agent`
2. **Properly annotate state fields** with reducers like `add_messages` and `operator.add`
3. **Store non-serializable objects outside of state** (like your kg_builder)
4. **Use `@tool` decorator** for all tool definitions with proper type hints
5. **Implement streaming execution** for better real-time feedback
6. **Use `Command` objects** for advanced routing scenarios
7. **Add proper error handling and checkpointing** with MemorySaver

These changes will ensure your implementation is compatible with the latest LangGraph (0.5.3) and LangChain patterns, providing better performance, maintainability, and access to the latest features like streaming, checkpointing, and improved observability.

# LangGraph Implementation Updates - Complete Integration File

T√¥i s·∫Ω t·ªïng h·ª£p t·∫•t c·∫£ c√°c thay ƒë·ªïi c·∫ßn thi·∫øt cho implementation LangGraph c·ªßa b·∫°n th√†nh m·ªôt file ho√†n ch·ªânh. File n√†y s·∫Ω thay th·∫ø ph·∫ßn multi-agent system trong plan.md c·ªßa b·∫°n v·ªõi c√°c pattern m·ªõi nh·∫•t.

```python
# src/agents/langgraph_agents.py
"""
Updated Multi-Agent System using Latest LangGraph Patterns
This implementation follows LangGraph 0.5.3 best practices
"""

import json
import uuid
from typing import Dict, Any, List, Optional, Annotated, Literal
from typing_extensions import TypedDict
import operator
import asyncio

from langchain_core.messages import BaseMessage, AnyMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent, ToolNode
from langgraph.types import Command
from pydantic import BaseModel, Field

# Import your existing components
from ..knowledge_graph.graph_builder import EnhancedKnowledgeGraphBuilder
from ..analysis.code_generator import SmartCodeGenerator
from ..analysis.execution_engine import AnalysisExecutionEngine


# ============================================
# PART 1: STATE DEFINITION WITH PROPER ANNOTATIONS
# ============================================

class AnalysisState(TypedDict):
    """
    State for multi-agent analysis workflow.
    Uses proper annotations for reducers and state management.
    """
    # Message history with add_messages reducer for proper accumulation
    messages: Annotated[List[AnyMessage], add_messages]
    
    # Dataset information
    dataset_info: Dict[str, Any]
    
    # Knowledge graph metadata (not the actual object)
    kg_metadata: Dict[str, Any]
    
    # Results from each agent
    exploration_results: Dict[str, Any]
    relationship_results: Dict[str, Any]
    pattern_results: Dict[str, Any]
    business_insights: Dict[str, Any]
    
    # Workflow control
    current_agent: str
    analysis_complete: bool
    
    # Loop counter with operator.add for incrementing
    loop_step: Annotated[int, operator.add]
    
    # Error tracking
    errors: List[str]


# ============================================
# PART 2: KNOWLEDGE GRAPH MANAGER (Singleton Pattern)
# ============================================

class KnowledgeGraphManager:
    """
    Singleton manager for knowledge graph access.
    Stores non-serializable objects outside of LangGraph state.
    """
    _instance = None
    _kg_builder = None
    _code_generator = None
    _execution_engine = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def initialize(self, kg_builder: EnhancedKnowledgeGraphBuilder):
        """Initialize with knowledge graph and create associated components"""
        self._kg_builder = kg_builder
        self._code_generator = SmartCodeGenerator(kg_builder.graph)
        self._execution_engine = AnalysisExecutionEngine()
    
    @property
    def kg_builder(self):
        return self._kg_builder
    
    @property
    def code_generator(self):
        return self._code_generator
    
    @property
    def execution_engine(self):
        return self._execution_engine


# ============================================
# PART 3: TOOL DEFINITIONS WITH @tool DECORATOR
# ============================================

# Define structured inputs for tools
class KGQueryInput(BaseModel):
    """Input schema for knowledge graph queries"""
    query: str = Field(description="Natural language query about the knowledge graph")
    filter_type: Optional[str] = Field(default=None, description="Filter by relationship type")
    max_results: int = Field(default=20, description="Maximum number of results")

class AnalysisRequestInput(BaseModel):
    """Input schema for analysis requests"""
    intent: str = Field(description="Analysis intent (e.g., 'correlation analysis')")
    table_focus: Optional[str] = Field(default=None, description="Specific table to focus on")
    columns: Optional[List[str]] = Field(default=None, description="Specific columns to analyze")


@tool(args_schema=KGQueryInput)
def query_knowledge_graph(query: str, filter_type: Optional[str] = None, max_results: int = 20) -> str:
    """
    Query the knowledge graph for relationship information and structure.
    
    This tool allows you to explore the knowledge graph by searching for
    relationships, finding connected nodes, and understanding data structure.
    """
    try:
        kg_manager = KnowledgeGraphManager.get_instance()
        kg_builder = kg_manager.kg_builder
        
        if not kg_builder:
            return json.dumps({"error": "Knowledge graph not initialized"})
        
        results = {"query": query, "results": []}
        
        # Handle different query types
        if "relationships" in query.lower():
            relationships = kg_builder.get_relationships()
            
            # Apply filter if specified
            if filter_type:
                relationships = [r for r in relationships if r['relationship'] == filter_type]
            
            results["results"] = relationships[:max_results]
            results["total_count"] = len(relationships)
            
        elif "related" in query.lower():
            # Extract column reference from query
            words = query.split()
            column_candidates = [word for word in words if '.' in word and 'COLUMN:' in word]
            
            if column_candidates:
                column = column_candidates[0]
                related = kg_builder.find_related_columns(column, max_distance=2)
                results["results"] = related[:max_results]
                
        elif "stats" in query.lower() or "statistics" in query.lower():
            results["results"] = {
                "total_nodes": kg_builder.graph.number_of_nodes(),
                "total_edges": kg_builder.graph.number_of_edges(),
                "node_types": dict(kg_builder.graph.nodes(data='type')),
                "relationship_types": list(set(data.get('relationship', 'unknown') 
                                             for _, _, data in kg_builder.graph.edges(data=True)))
            }
        
        return json.dumps(results, indent=2, default=str)
        
    except Exception as e:
        return json.dumps({"error": str(e), "query": query})


@tool(args_schema=AnalysisRequestInput)
def execute_data_analysis(intent: str, table_focus: Optional[str] = None, 
                         columns: Optional[List[str]] = None) -> str:
    """
    Execute data analysis code based on the specified intent.
    
    This tool generates and executes appropriate analysis code for tasks like
    correlation analysis, clustering, anomaly detection, etc.
    """
    try:
        kg_manager = KnowledgeGraphManager.get_instance()
        
        # Prepare dataset info from state/context
        dataset_info = {
            "dataset_name": "analysis_dataset",
            "tables": {},
            "primary_table": table_focus
        }
        
        # Generate analysis code
        code = kg_manager.code_generator.generate_analysis_code(
            intent=intent,
            dataset_info=dataset_info,
            kg_context={"suggested_columns": columns} if columns else None
        )
        
        # Execute the code
        # Note: In real implementation, you'd pass actual data here
        result = kg_manager.execution_engine.execute_analysis(
            code=code,
            data={},  # Would be populated with actual DataFrames
            context={"intent": intent}
        )
        
        # Format results
        output = {
            "intent": intent,
            "success": result['success'],
            "execution_time": result['execution_time'],
            "key_findings": []
        }
        
        if result['success']:
            # Extract key insights from stdout
            stdout_lines = result['stdout'].split('\n')
            for line in stdout_lines:
                if any(keyword in line.lower() for keyword in 
                      ['found', 'detected', 'correlation', 'cluster', 'anomaly']):
                    output['key_findings'].append(line.strip())
        else:
            output['error'] = result.get('error_message', 'Unknown error')
        
        return json.dumps(output, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e), "intent": intent})


# ============================================
# PART 4: AGENT NODE FUNCTIONS
# ============================================

def create_llm(model_name: str = "gpt-4", temperature: float = 0):
    """Create LLM with flexible initialization"""
    return init_chat_model(
        model=f"openai:{model_name}",
        temperature=temperature,
        model_provider="openai"
    )


def data_exploration_node(state: AnalysisState) -> Dict[str, Any]:
    """
    Data exploration agent node.
    Performs comprehensive exploratory data analysis.
    """
    # Create the agent with specific prompt and tools
    llm = create_llm()
    tools = [query_knowledge_graph, execute_data_analysis]
    
    exploration_prompt = """You are an expert data scientist specializing in exploratory data analysis.
    You excel at understanding data structure, identifying quality issues, and discovering initial patterns.
    
    Your current task:
    1. Query the knowledge graph to understand dataset structure
    2. Analyze basic statistics and data quality
    3. Identify key variables and their distributions
    4. Generate initial insights about the data
    
    Be thorough but concise. Focus on actionable findings."""
    
    agent = create_react_agent(llm, tools, prompt=exploration_prompt)
    
    # Prepare input messages
    messages = state["messages"] + [
        HumanMessage(content=f"Analyze dataset: {state['dataset_info'].get('name', 'Unknown')}. "
                           f"Tables: {list(state['dataset_info'].get('tables', {}).keys())}")
    ]
    
    # Execute agent
    response = agent.invoke({"messages": messages})
    
    # Extract findings from response
    exploration_results = {
        "analysis_complete": True,
        "execution_time": 0,  # Would be tracked in real implementation
        "key_findings": extract_findings_from_messages(response["messages"]),
        "data_quality_issues": extract_quality_issues(response["messages"]),
        "recommended_analyses": extract_recommendations(response["messages"])
    }
    
    # Return state updates
    return {
        "messages": response["messages"],
        "exploration_results": exploration_results,
        "current_agent": "relationship_analyst",
        "loop_step": 1
    }


def relationship_analysis_node(state: AnalysisState) -> Dict[str, Any]:
    """
    Relationship analysis agent node.
    Focuses on discovering and analyzing relationships in the data.
    """
    llm = create_llm()
    tools = [query_knowledge_graph, execute_data_analysis]
    
    relationship_prompt = """You are a specialist in finding hidden relationships and patterns in data.
    You use knowledge graphs to understand how different variables connect and influence each other.
    
    Your current task:
    1. Query the knowledge graph for all discovered relationships
    2. Analyze correlation patterns between numeric variables
    3. Identify foreign key relationships between tables
    4. Evaluate the strength and significance of relationships
    
    Build on the previous exploration results to provide deeper insights."""
    
    agent = create_react_agent(llm, tools, prompt=relationship_prompt)
    
    # Include context from previous analysis
    context_message = f"Previous exploration found: {state['exploration_results'].get('key_findings', [])[:3]}"
    messages = state["messages"] + [HumanMessage(content=context_message)]
    
    # Execute agent
    response = agent.invoke({"messages": messages})
    
    # Extract relationship insights
    relationship_results = {
        "analysis_complete": True,
        "relationships_found": extract_relationships(response["messages"]),
        "correlation_insights": extract_correlations(response["messages"]),
        "cross_table_dependencies": extract_dependencies(response["messages"])
    }
    
    return {
        "messages": response["messages"],
        "relationship_results": relationship_results,
        "current_agent": "pattern_miner",
        "loop_step": 1
    }


def pattern_mining_node(state: AnalysisState) -> Dict[str, Any]:
    """
    Pattern mining agent node.
    Discovers clusters, anomalies, and hidden patterns.
    """
    llm = create_llm()
    tools = [query_knowledge_graph, execute_data_analysis]
    
    pattern_prompt = """You are an expert in unsupervised learning and pattern recognition.
    You specialize in clustering analysis, outlier detection, and identifying unusual patterns.
    
    Your current task:
    1. Perform clustering analysis to identify natural groups
    2. Detect outliers and anomalies using multiple methods
    3. Identify frequent patterns and associations
    4. Analyze temporal patterns if time-based data exists
    
    Use insights from previous analyses to guide your pattern discovery."""
    
    agent = create_react_agent(llm, tools, prompt=pattern_prompt)
    
    # Build on previous findings
    context = (f"Key relationships: {state['relationship_results'].get('relationships_found', [])[:2]}. "
              f"Focus on these areas for pattern discovery.")
    messages = state["messages"] + [HumanMessage(content=context)]
    
    # Execute agent
    response = agent.invoke({"messages": messages})
    
    # Extract pattern insights
    pattern_results = {
        "analysis_complete": True,
        "clusters_identified": extract_clusters(response["messages"]),
        "anomalies_detected": extract_anomalies(response["messages"]),
        "patterns_discovered": extract_patterns(response["messages"])
    }
    
    return {
        "messages": response["messages"],
        "pattern_results": pattern_results,
        "current_agent": "business_synthesizer",
        "loop_step": 1
    }


def business_synthesis_node(state: AnalysisState) -> Dict[str, Any]:
    """
    Business synthesis agent node.
    Synthesizes all findings into actionable business insights.
    """
    llm = create_llm(temperature=0.3)  # Slightly higher temperature for creativity
    
    synthesis_prompt = """You are a senior business analyst who excels at translating complex
    data analysis results into clear, actionable business insights.
    
    Your task is to synthesize all previous analysis results:
    1. Review findings from data exploration, relationship analysis, and pattern mining
    2. Identify the most significant discoveries for business impact
    3. Generate specific, actionable recommendations
    4. Prioritize insights by potential business value
    5. Suggest next steps and further analyses if needed
    
    Be concise but comprehensive. Focus on business value and actionability."""
    
    # For synthesis, we don't need tools - just LLM reasoning
    agent = create_react_agent(llm, [], prompt=synthesis_prompt)
    
    # Prepare comprehensive context
    synthesis_context = f"""
    Dataset: {state['dataset_info'].get('name', 'Unknown')}
    Business Context: {state['dataset_info'].get('business_context', 'General analysis')}
    
    Key Findings Summary:
    - Exploration: {state['exploration_results'].get('key_findings', [])[:3]}
    - Relationships: {state['relationship_results'].get('relationships_found', [])[:3]}
    - Patterns: {state['pattern_results'].get('patterns_discovered', [])[:3]}
    
    Synthesize these findings into business insights and recommendations.
    """
    
    messages = state["messages"] + [HumanMessage(content=synthesis_context)]
    
    # Execute synthesis
    response = agent.invoke({"messages": messages})
    
    # Extract business insights
    business_insights = {
        "analysis_complete": True,
        "executive_summary": extract_summary(response["messages"]),
        "key_insights": extract_insights(response["messages"]),
        "recommendations": extract_recommendations(response["messages"]),
        "next_steps": extract_next_steps(response["messages"])
    }
    
    return {
        "messages": response["messages"],
        "business_insights": business_insights,
        "analysis_complete": True,
        "loop_step": 1
    }


# ============================================
# PART 5: EXTRACTION HELPER FUNCTIONS
# ============================================

def extract_findings_from_messages(messages: List[BaseMessage]) -> List[str]:
    """Extract key findings from agent messages"""
    findings = []
    for message in messages:
        if hasattr(message, 'content'):
            content = message.content
            # Simple extraction logic - in production, use NLP
            lines = content.split('\n')
            for line in lines:
                if any(indicator in line.lower() for indicator in 
                      ['found', 'discovered', 'identified', 'detected']):
                    findings.append(line.strip())
    return findings[:5]  # Top 5 findings


def extract_quality_issues(messages: List[BaseMessage]) -> List[str]:
    """Extract data quality issues from messages"""
    issues = []
    for message in messages:
        if hasattr(message, 'content'):
            content = message.content.lower()
            if 'missing' in content or 'null' in content or 'quality' in content:
                # Extract relevant sentences
                sentences = content.split('.')
                for sentence in sentences:
                    if any(word in sentence for word in ['missing', 'null', 'quality', 'duplicate']):
                        issues.append(sentence.strip())
    return issues[:3]


def extract_recommendations(messages: List[BaseMessage]) -> List[str]:
    """Extract recommendations from messages"""
    recommendations = []
    for message in messages:
        if hasattr(message, 'content'):
            content = message.content
            lines = content.split('\n')
            for i, line in enumerate(lines):
                if any(word in line.lower() for word in ['recommend', 'suggest', 'should', 'could']):
                    recommendations.append(line.strip())
    return recommendations[:5]


# Similar extraction functions for other types...
def extract_relationships(messages): return []
def extract_correlations(messages): return []
def extract_dependencies(messages): return []
def extract_clusters(messages): return []
def extract_anomalies(messages): return []
def extract_patterns(messages): return []
def extract_summary(messages): return ""
def extract_insights(messages): return []
def extract_next_steps(messages): return []


# ============================================
# PART 6: ORCHESTRATOR WITH CONDITIONAL ROUTING
# ============================================

class LangGraphMultiAgentOrchestrator:
    """
    Main orchestrator for multi-agent analysis using LangGraph.
    Implements latest patterns including conditional routing and streaming.
    """
    
    def __init__(self, kg_builder: EnhancedKnowledgeGraphBuilder):
        # Initialize the knowledge graph manager
        KnowledgeGraphManager.get_instance().initialize(kg_builder)
        
        # Store metadata for state
        self.kg_metadata = {
            'nodes': kg_builder.graph.number_of_nodes(),
            'edges': kg_builder.graph.number_of_edges(),
            'relationships': len(kg_builder.get_relationships())
        }
        
        # Initialize memory and workflow
        self.memory = MemorySaver()
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build the LangGraph workflow with conditional routing"""
        # Create state graph
        workflow = StateGraph(AnalysisState)
        
        # Add all nodes
        workflow.add_node("data_explorer", data_exploration_node)
        workflow.add_node("relationship_analyst", relationship_analysis_node)
        workflow.add_node("pattern_miner", pattern_mining_node)
        workflow.add_node("business_synthesizer", business_synthesis_node)
        
        # Entry point
        workflow.add_edge(START, "data_explorer")
        
        # Conditional routing after exploration
        def route_after_exploration(state: AnalysisState) -> str:
            if state.get("exploration_results", {}).get("analysis_complete"):
                return "relationship_analyst"
            return END
        
        workflow.add_conditional_edges(
            "data_explorer",
            route_after_exploration
        )
        
        # Sequential flow for remaining agents
        workflow.add_edge("relationship_analyst", "pattern_miner")
        workflow.add_edge("pattern_miner", "business_synthesizer")
        workflow.add_edge("business_synthesizer", END)
        
        # Compile with memory
        return workflow.compile(checkpointer=self.memory)
    
    async def analyze_dataset_async(self, dataset_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute multi-agent analysis asynchronously with streaming.
        """
        # Prepare initial state
        initial_state = {
            "messages": [HumanMessage(content="Starting comprehensive data analysis")],
            "dataset_info": dataset_info,
            "kg_metadata": self.kg_metadata,
            "exploration_results": {},
            "relationship_results": {},
            "pattern_results": {},
            "business_insights": {},
            "current_agent": "data_explorer",
            "analysis_complete": False,
            "loop_step": 0,
            "errors": []
        }
        
        # Configure thread for conversation memory
        config = {"configurable": {"thread_id": str(uuid.uuid4())}}
        
        # Track execution
        execution_log = []
        
        try:
            # Stream execution for real-time updates
            async for event in self.workflow.astream(initial_state, config):
                current_agent = event.get("current_agent", "unknown")
                print(f"\nü§ñ Agent Active: {current_agent}")
                
                # Log the event
                execution_log.append({
                    "timestamp": str(uuid.uuid4()),  # Would use real timestamp
                    "agent": current_agent,
                    "event": event
                })
                
                # Show latest message if available
                if "messages" in event and event["messages"]:
                    last_message = event["messages"][-1]
                    if hasattr(last_message, 'content'):
                        print(f"üí¨ {last_message.content[:200]}...")
                
                # Show results as they complete
                for result_key in ["exploration_results", "relationship_results", 
                                 "pattern_results", "business_insights"]:
                    if result_key in event and event[result_key]:
                        print(f"‚úÖ Completed: {result_key}")
            
            # Get final state
            final_state = execution_log[-1]["event"] if execution_log else initial_state
            
            # Compile comprehensive results
            return self._compile_results(final_state, execution_log)
            
        except Exception as e:
            print(f"‚ùå Analysis failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "partial_results": execution_log
            }
    
    def analyze_dataset(self, dataset_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Synchronous wrapper for dataset analysis.
        """
        # Run async function in sync context
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self.analyze_dataset_async(dataset_info))
        finally:
            loop.close()
    
    def _compile_results(self, final_state: Dict[str, Any], 
                        execution_log: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compile final results from analysis"""
        return {
            'success': True,
            'analysis_results': {
                'exploration': final_state.get('exploration_results', {}),
                'relationships': final_state.get('relationship_results', {}),
                'patterns': final_state.get('pattern_results', {}),
                'business_insights': final_state.get('business_insights', {})
            },
            'execution_summary': {
                'total_agents': 4,
                'completed_agents': sum(1 for key in ['exploration_results', 
                                                     'relationship_results',
                                                     'pattern_results', 
                                                     'business_insights']
                                      if final_state.get(key, {}).get('analysis_complete')),
                'total_messages': len(final_state.get('messages', [])),
                'analysis_complete': final_state.get('analysis_complete', False)
            },
            'knowledge_graph_stats': self.kg_metadata,
            'execution_log': execution_log[-10:]  # Last 10 events
        }


# ============================================
# PART 7: ADVANCED ROUTING WITH COMMAND OBJECTS
# ============================================

def create_supervisor_workflow():
    """
    Example of advanced workflow with supervisor pattern and Command routing.
    """
    
    class SupervisorState(TypedDict):
        messages: Annotated[List[AnyMessage], add_messages]
        next_agent: str
        completed_agents: List[str]
        max_iterations: int
        current_iteration: Annotated[int, operator.add]
    
    def supervisor_node(state: SupervisorState) -> Command[Literal["explorer", "analyst", "synthesizer", END]]:
        """
        Supervisor that decides which agent to call next using Command objects.
        """
        completed = set(state.get("completed_agents", []))
        iteration = state.get("current_iteration", 0)
        
        # Check termination conditions
        if iteration >= state.get("max_iterations", 5):
            return Command(goto=END)
        
        # Decide next agent based on what's been completed
        if "explorer" not in completed:
            next_agent = "explorer"
        elif "analyst" not in completed:
            next_agent = "analyst"
        elif "synthesizer" not in completed:
            next_agent = "synthesizer"
        else:
            return Command(goto=END)
        
        # Return command with state updates
        return Command(
            goto=next_agent,
            update={
                "next_agent": next_agent,
                "current_iteration": 1,
                "messages": [AIMessage(content=f"Routing to {next_agent}")]
            }
        )
    
    # Build supervisor workflow
    supervisor_workflow = StateGraph(SupervisorState)
    
    # Add nodes
    supervisor_workflow.add_node("supervisor", supervisor_node)
    supervisor_workflow.add_node("explorer", lambda s: {"completed_agents": s.get("completed_agents", []) + ["explorer"]})
    supervisor_workflow.add_node("analyst", lambda s: {"completed_agents": s.get("completed_agents", []) + ["analyst"]})
    supervisor_workflow.add_node("synthesizer", lambda s: {"completed_agents": s.get("completed_agents", []) + ["synthesizer"]})
    
    # Set up routing
    supervisor_workflow.add_edge(START, "supervisor")
    
    # Each agent reports back to supervisor
    for agent in ["explorer", "analyst", "synthesizer"]:
        supervisor_workflow.add_edge(agent, "supervisor")
    
    return supervisor_workflow.compile()


# ============================================
# PART 8: USAGE EXAMPLE
# ============================================

if __name__ == "__main__":
    # Example usage with the new implementation
    
    # Assuming you have your knowledge graph builder ready
    from ..knowledge_graph.graph_builder import EnhancedKnowledgeGraphBuilder
    
    # Load your data
    import pandas as pd
    datasets = {
        'customers': pd.DataFrame({'id': [1, 2, 3], 'name': ['A', 'B', 'C']}),
        'orders': pd.DataFrame({'id': [1, 2, 3], 'customer_id': [1, 2, 1]})
    }
    
    # Build knowledge graph
    kg_builder = EnhancedKnowledgeGraphBuilder()
    kg_builder.add_dataset(datasets, "demo_dataset")
    
    # Initialize orchestrator
    orchestrator = LangGraphMultiAgentOrchestrator(kg_builder)
    
    # Prepare dataset info
    dataset_info = {
        'name': 'demo_dataset',
        'tables': {name: {'columns': list(df.columns)} for name, df in datasets.items()},
        'business_context': 'Customer order analysis',
        'data': datasets
    }
    
    # Run analysis
    print("üöÄ Starting Multi-Agent Analysis...")
    results = orchestrator.analyze_dataset(dataset_info)
    
    # Display results
    print("\nüìä Analysis Results:")
    print(f"Success: {results['success']}")
    print(f"Agents completed: {results['execution_summary']['completed_agents']}/4")
    print(f"Total messages: {results['execution_summary']['total_messages']}")
    
    # Show business insights
    if results['success']:
        insights = results['analysis_results']['business_insights']
        print(f"\nüí° Executive Summary: {insights.get('executive_summary', 'N/A')}")
        print(f"\nüéØ Key Recommendations:")
        for rec in insights.get('recommendations', [])[:3]:
            print(f"  ‚Ä¢ {rec}")
```

## T√≥m t·∫Øt c√°c thay ƒë·ªïi ch√≠nh

File tr√™n ƒë√£ t√≠ch h·ª£p t·∫•t c·∫£ c√°c c·∫≠p nh·∫≠t c·∫ßn thi·∫øt cho LangGraph m·ªõi nh·∫•t:

1. **State Management**: S·ª≠ d·ª•ng `Annotated` v·ªõi c√°c reducer functions (`add_messages`, `operator.add`)

2. **Tool Pattern**: D√πng decorator `@tool` v·ªõi schema validation th√¥ng qua Pydantic

3. **Agent Creation**: Thay `create_openai_tools_agent` b·∫±ng `create_react_agent`

4. **Non-serializable Objects**: L∆∞u tr·ªØ kg_builder ngo√†i state th√¥ng qua singleton pattern

5. **Streaming Support**: H·ªó tr·ª£ async streaming v·ªõi `astream()`

6. **Command Objects**: V√≠ d·ª• v·ªÅ routing n√¢ng cao v·ªõi `Command` objects

7. **Memory/Checkpointing**: T√≠ch h·ª£p `MemorySaver` cho state persistence

8. **Error Handling**: X·ª≠ l√Ω l·ªói v√† tracking trong state

B·∫°n c√≥ th·ªÉ thay th·∫ø ph·∫ßn multi-agent trong plan.md c·ªßa m√¨nh b·∫±ng implementation n√†y ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi LangGraph 0.5.3 m·ªõi nh·∫•t.